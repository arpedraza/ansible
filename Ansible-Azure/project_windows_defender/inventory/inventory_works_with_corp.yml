#!/usr/bin/env python3
import json
import os
import sys
import time
import subprocess
from typing import Dict, Any, List, Tuple, Optional

# -------------------------
# Tuning knobs
# -------------------------
CACHE_FILE = os.path.expanduser("~/.azure_inventory_cache.json")
CACHE_TTL_SECONDS = 300          # 5 minutes
GRAPH_PAGE_SIZE = 1000
AZ_TIMEOUT_SECONDS = 30          # hard cap per az call

# -------------------------
# Helpers
# -------------------------
def az(cmd: List[str], timeout: int = AZ_TIMEOUT_SECONDS) -> subprocess.CompletedProcess:
    """
    Run Azure CLI command. Never hangs: bounded by timeout.
    """
    return subprocess.run(
        ["az"] + cmd,
        capture_output=True,
        text=True,
        timeout=timeout
    )

def empty_inventory() -> Dict[str, Any]:
    return {
        "_meta": {"hostvars": {}},
        "all": {"hosts": []},
        "linux": {"hosts": []},
        "windows": {"hosts": []},
        "windows_corp": {"hosts": []},
        "network_appliances": {"hosts": []}
    }

def load_cache_if_fresh() -> Optional[str]:
    try:
        if os.path.exists(CACHE_FILE) and (time.time() - os.path.getmtime(CACHE_FILE) < CACHE_TTL_SECONDS):
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                return f.read()
    except Exception:
        pass
    return None

def save_cache(payload: Dict[str, Any]) -> None:
    try:
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(payload, f)
    except Exception:
        # Cache failure should never break inventory output
        pass

def graph_query_all(query: str, page_size: int = GRAPH_PAGE_SIZE) -> List[Dict[str, Any]]:
    """
    Fetch all results for an Azure Resource Graph query using skipToken pagination.
    Returns [] if anything goes wrong (inventory will degrade gracefully).
    """
    results: List[Dict[str, Any]] = []
    skip_token: Optional[str] = None

    while True:
        cmd = ["graph", "query", "-q", query, "--first", str(page_size), "-o", "json"]
        if skip_token:
            cmd += ["--skip-token", skip_token]

        try:
            r = az(cmd, timeout=AZ_TIMEOUT_SECONDS)
        except subprocess.TimeoutExpired:
            return []

        if r.returncode != 0 or not r.stdout.strip():
            return []

        try:
            payload = json.loads(r.stdout)
        except json.JSONDecodeError:
            return []

        data = payload.get("data") or []
        results.extend(data)

        # Azure CLI typically returns skipToken when more pages exist
        skip_token = payload.get("skipToken") or payload.get("skip_token") or None
        if not skip_token:
            break

    return results

APPLIANCE_PUBLISHERS = {
    "citrix",
    "barracudanetworks",
    "f5-networks",
    "paloaltonetworks",
    "fortinet",
    "sophos",
    "checkpoint",
    "cisco"
}

# -------------------------
# Main
# -------------------------
def main() -> None:
    # 1) Serve cache immediately if fresh
    cached = load_cache_if_fresh()
    if cached is not None:
        print(cached)
        return

    # 2) Two bulk ARG queries: VMs + NICs (fastest approach)
    vm_query = """
Resources
| where type == 'microsoft.compute/virtualmachines'
| project
    name,
    resourceGroup,
    location,
    osType     = tostring(properties.storageProfile.osDisk.osType),
    publisher  = tostring(properties.storageProfile.imageReference.publisher),
    offer      = tostring(properties.storageProfile.imageReference.offer),
    sku        = tostring(properties.storageProfile.imageReference.sku),
    nicId      = tostring(properties.networkProfile.networkInterfaces[0].id),
    computerName = tostring(properties.osProfile.computerName),
    domainFqdn = tostring(tags['DomainFQDN'])
"""

    nic_query = """
Resources
| where type == 'microsoft.network/networkinterfaces'
| project
    id,
    privateIp = tostring(properties.ipConfigurations[0].properties.privateIPAddress)
"""

    vms = graph_query_all(vm_query)
    nics = graph_query_all(nic_query)

    # If Azure CLI / graph fails, return empty inventory (donâ€™t break Ansible)
    if not vms:
        inv = empty_inventory()
        save_cache(inv)
        print(json.dumps(inv, indent=2))
        return

    # 3) Build NIC -> IP map locally (O(1) lookups)
    nic_ip_map: Dict[str, Optional[str]] = {}
    for nic in nics:
        nic_id = nic.get("id")
        if nic_id:
            nic_ip_map[nic_id] = (nic.get("privateIp") or "").strip() or None

    # 4) Build inventory
    inventory = empty_inventory()

    for vm in vms:
        vm_name = (vm.get("name") or "").strip()
        if not vm_name:
            continue
        
        computer_name = (vm.get("computerName") or vm_name).strip()
        domain_fqdn = (vm.get("domainFqdn") or "").strip()

        if vm.get("osType") == "Windows" and domain_fqdn:
          inventory_name = f"{computer_name}.{domain_fqdn}".lower()
        else:
          inventory_name = vm_name

        nic_id = vm.get("nicId")
        ip = nic_ip_map.get(nic_id)

        inventory["all"]["hosts"].append(inventory_name)
        inventory["_meta"]["hostvars"][inventory_name] = {
            "ansible_host": ip,          # Connection target (IP)
            "private_ip": ip,            # Metadata
            "resource_group": vm.get("resourceGroup"),
            "location": vm.get("location"),
            "os_type": vm.get("osType"),
            "nic_id": nic_id,
            "azure_vm_name": vm_name,
            "domain_fqdn": domain_fqdn or None
        }

        publisher = (vm.get("publisher") or "").lower()
        is_appliance = any(p in publisher for p in APPLIANCE_PUBLISHERS)

        if vm.get("osType") == "Linux" and not is_appliance:
            inventory["linux"]["hosts"].append(inventory_name)

        elif vm.get("osType") == "Linux" and is_appliance:
            inventory["network_appliances"]["hosts"].append(inventory_name)

        else:
            inventory["windows"]["hosts"].append(inventory_name)

            if domain_fqdn:
                inventory["windows_corp"]["hosts"].append(inventory_name)

    # 5) Cache + output
    save_cache(inventory)
    print(json.dumps(inventory, indent=2))

if __name__ == "__main__":
    main()

